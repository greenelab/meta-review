## Main text {.page_break_before}

### Introduction

Open research – which includes sharing code, data, and manuscripts – benefits the researchers who engage in open practices [@doi:10.7554/eLife.16800], their scientific peers, and the public.
`TODO: more references needed`
Here we describe the benefits of writing review articles openly, where the planning, organizing, writing, and editing occur collaboratively in a public forum where participants are free to join as they wish.
Reviews presenting the state of the art in a scientific field are often prepared by a single research group or a small team of colleagues.
In contrast, broadly opening the writing process to anyone engaged in the topic can help maximize the review's value by facilitating the representation of diverse opinions and the broad coverage of relevant research.
Review authors can engage with the authors of original research to clarify their methods and results and present them accurately (for example, @url:https://github.com/greenelab/deep-review/issues/213).
`TODO: need archival issue link`
In addition, discussing manuscripts in the open provides one form of pre- and post-publication peer review `TODO: define this or provide a reference?`, incentivizing the reviews with potential manuscript authorship.
However, inviting wide authorship brings many technical and social challenges such as how to fairly distribute credit, coordinate the scientific content, and collaboratively manage extensive reference lists.

We present solutions to these challenges based on our recent experience leading a collaborative review "Opportunities And Obstacles For Deep Learning In Biology And Medicine" [@doi:10.1101/142760].
Our review attracted 27 authors from 20 different institutions who were not determined in advance.
`TODO: confirm institution count`
We wrote entirely in the open without restrictions on who was welcome to contribute.
Although we requested that some authors participate for their specific expertise, most discovered the manuscript organically through conferences or social media and independently decided to contribute.
`TODO: confirm "most"`
To coordinate this effort, we developed a manuscript writing process using the Markdown language, the GitHub software development platform [@url:https://github.com/greenelab/deep-review/], and our new Manubot tool [@url:https://github.com/greenelab/manubot-rootstock; @url:https://github.com/greenelab/manubot] for automating manuscript generation.

### Manubot

We developed Manubot, a system for writing scholarly manuscripts via GitHub.
With Manubot, manuscripts are written as plain-text markdown files, which is well suited for version control using git.
The markdown standard itself provides limited yet crucial formatting syntax, including the ability to embed images and format text via bold, italics, hyperlinks, headers, inline code, codeblocks, blockquotes, and numbered or bulleted lists.
In addition, Manubot relies on Pandoc's extended markdown to enable citations, tables, captions, and equations specified using the popular TeX math syntax.

Manubot includes an additional layer of citation processing, currently unique to the system.
All citations point to a standard identifier, for which Manubot automatically retrieves bibliographic metadata.
Currently, citations to DOIs (Digital Object Identifiers), PubMed identifiers, arXiv identifiers, and URLs (web addresses) are supported.
Metadata is retrieved using DOI [Content Negotiation](https://citation.crosscite.org/docs.html), NCBI's [Citation Exporter](https://www.ncbi.nlm.nih.gov/pmc/tools/ctxp/), the [arXiv API](https://arxiv.org/help/api/index), or [Greycite](http://greycite.knowledgeblog.org/) [@arxiv:1304.7151v1].
Metadata is exported to [CSL JSON Items](http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html#items), an open standard that's widely supported by reference managers [@doi:10.1007/978-3-319-00026-8_8; @doi:10.1080/02763869.2012.641841].
In cases where automatic retrieval of metadata fails or produces incorrect references -- which is most common for URL citations -- users can manually provide the correct CSL JSON.

The Manubot formats bibliographies according to a [Citation Style Language](http://citationstyles.org/) (CSL) specification.
As a result, users can choose from thousands of existing CSL styles or use Manubot's default style.
Styles define how references are constructed from bibliographic metadata, controlling layout details such as the max number of authors to list per reference.
Thousands of journals have [predefined styles](http://editor.citationstyles.org/searchByName/).
As a result, adopting the specific bibliographic format required by a journal usually just requires specifying the style's source URL in the Manubot's configuration.

Manubot uses [Pandoc](https://pandoc.org/) to convert manuscripts from markdown source to HTML (and optionally DOCX) output.
Pandoc supports conversion between a large number of formats, offering Manubot users broad interoperability and preventing platform lock-in.
For example, Manubot intentionally does not rely on LaTeX, although users are free to export manuscripts to LaTeX themselves.
In a future release, Pandoc [will support](https://github.com/greenelab/manubot-rootstock/pull/51#issuecomment-321949622) export to [Journal Article Tag Suite](https://jats.nlm.nih.gov/) (JATS).
JATS is a standard XML format for scholarly articles that is used by publishers, archives, and text miners [@url:http://www.niso.org/standards/z39-96-2015/; @doi:10.6087/kcse.2014.1.99; @doi:10.1080/00987913.2012.10765464].
For now however, the primary Manubot output is HTML intended to be viewed in a web browser.
From the rendered HTML, Manubot also exports to PDF.
### Contribution workflow

There are many existing collaborative writing platforms ranging from rich text editors, which support Microsoft Word documents or similar formats, to LaTeX-based systems for more technical writing [@doi:10.1038/514127a; @url:https://www.overleaf.com/; @url:https://www.authorea.com/].
These platforms ideally offer version control, multiple permission levels, or other functionality to support multi-author document editing.
Although they work well for editing, they lack sufficient features for managing a collaborative manuscript and attributing precise credit, which are important for open writing.

We adopted standard software development strategies in order to enable any contributor to edit any part of the manuscript but enforce discussion and review of all proposed changes.
The GitHub platform provided support for organizing and editing the manuscript.
We used GitHub _issues_ for organization, opening a new issue for each paper under consideration.
Within the issue, contributors summarized the research, discussed it (sometimes with the original authors), and assessed its relevance to the review.
Issues also served as an open to-do list and a forum for debating the main message, themes, and topics of the review.

GitHub and the underlying git version control system [@doi:10.1371/journal.pcbi.1004668; @doi:10.1371/journal.pcbi.1004947] also structured the writing process.
The official version of the manuscript is _forked_ by individual contributors.
A contributor then adds and revises files, grouping these changes into _commits_.
When the changes are ready to be reviewed, the series of commits are submitted as a _pull request_ through GitHub, which notifies other authors of the pending changes.
GitHub's review interface allows anyone to comment on the changes, globally or at specific lines, asking questions or requesting modifications as depicted in [@url:https://medium.com/towards-data-science/opportunities-and-obstacles-for-deep-learning-in-biology-and-medicine-6ec914fe18c2].
Conversations during review can reference other pull requests, issues, or authors, linking the relevant people and content.
Reviewing batches of revisions that focus on a single theme is more efficient than independently discussing isolated comments and edits, and it helps maintain consistent content and tone across different authors and reviewers.
Once all requested modifications are made, the manuscript maintainers, a subset of authors with elevated GitHub permissions, formally approve the pull request and merge the changes into the official version.
`TODO: need a figure with a flowchart showing this process`
The entire process can be orchestrated through GitHub with a web browser if a contributor does not want to use a git client on their own computer.

We found that this workflow was an effective compromise between fully unrestricted editing and a more heavily-structured approach that limits the authors or the sections they can edit.
In addition, authors are associated with their commits, which makes it easy for contributors to receive credit for their work and helps prevent ghostwriting [@doi:10.1371/journal.pmed.1000023].
The GitHub contributors page summarizes all edits and commits from each author, providing aggregated information that is not available on other collaborative writing platforms.
Because our writing process, like others backed by the open git version control system [@url:https://www.overleaf.com/; @url:https://www.authorea.com/], tracks the complete commit history, it also enables detailed retrospective contribution analysis.
`TODO: confirm Overleaf and Authorea provide this type of git integration versus something more coarse`

### Authorship

To determine authorship we followed the International Committee of Medical Journal Editors (ICMJE) guidelines and used GitHub to track contributions.
ICMJE recommends authors substantially contribute to, draft, approve, and agree to be accountable for the manuscript [@url:http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html].
We acknowledged other contributors who did not meet all four criteria, including contributors who provided text but did not review and approve the complete manuscript.
`TODO: In Discussion, mention the challenge of determining contributions of potential authors who worked with someone else on the text but did not make their own commits`
Although these criteria provided a straightforward, equitable way to determine who would be an author, they did not produce a traditionally ordered author list.
In biomedical journals, the convention is that the first and last authors made the most substantial contributions to the manuscript.
This convention can be difficult to reconcile in a collaborative effort.
Using git, we could quantify the number of commits each author made or the number of sentences an author wrote or edited, but these metrics discount intellectual contributions such as discussing primary literature and reviewing pull requests.
However, there is no objective system to compare and weight the different types of contributions and produce an ordered author list.

To address this issue, we generalized the concept of "co-first" authorship, in which two or more authors are denoted as making equal contributions to a paper.
We defined four types of contributions [@doi:10.1101/142760], from major to minor, and reviewed the GitHub discussions and commits to assign authors to these categories.
A randomized algorithm then arbitrarily ordered authors within each contribution category, and we combined the category-specific author lists to produce a traditional ordering.
The randomization procedure was shared with the authors in advance (pre-registered) and run in a deterministic manner.
Given the same author contributions, it always produced the same ordered author list.
We annotated the author list to indicate that author order was partly randomized and emphasize that the order did not indicate one author contributed more than another from the same category.
`TODO: In Discussion, present alternative author ordering strategies and literature on contribution in collaborative projects`

### Discussion

Many others have embraced open science principles and piloted open approaches toward drug discovery [@doi:10.7554/eLife.26726; @doi:10.1371/journal.pmed.1002276], data management [@doi:10.1371/journal.pbio.1001195; @doi:10.1038/sdata.2016.18; @doi:10.1038/nbt.3516], and manuscript review [@doi:10.12688/f1000research.12037.1].
`TODO: need help deciding what related topics to include here and which references to use, these are arbitrary examples`
`TODO: more ideas in doi:10.7287/peerj.preprints.2711v2`
Several of these open science efforts are GitHub-based like our collaborative writing process.
The Journal of Open Source Software [@arxiv:1707.02264] and ReScience [@arxiv:1707.04393] journals rely on GitHub for peer review and hosting.
`TODO: describe Manubot related work here?` [@doi:10.7717/peerj-cs.112; @url:https://github.com/ewanmellor/gh-publisher]

There are potential limitations of our GitHub-based approach.
Because our review manuscript pertained to a computational topic, most of the authors had computational backgrounds, including previous experience with version control workflows and GitHub.
In other disciplines, collaborative writing via GitHub and Manubot could present a steeper barrier to entry and deter participants.
In addition, git carefully tracks all revisions to the manuscript text but not the surrounding conversations that take place through GitHub issues and pull requests.
These discussions must be archived to ensure that important decisions about the manuscript are preserved and authors receive credit for intellectual contributions that are not directly reflected in the manuscript's text.
GitHub supports programmatic access to issues, pull requests, and reviews so tracking these conversations is feasible in the future.

`TODO: cgreene paragraph on diversity, open science culture, possibly author disagreements and conduct`

Open writing presents new opportunities for scholarly communication.
`TODO: reference "paper of the future"? arXiv:1601.02927 doi:10.22541/au.149693987.70506124 doi:10.22541/au.148769949.92783646 http://blogs.nature.com/naturejobs/2017/06/01/techblog-c-titus-brown-predicting-the-paper-of-the-future`
Though it is still valuable to have versioned drafts of a review manuscript with digital identifiers, journal publication may not be the terminal endpoint for collaborative manuscripts.
After releasing the first version of our collaborative review [@doi:10.1101/142760], six new authors have contributed text and existing authors continue to discuss new literature, creating a living document [@url:https://github.com/greenelab/deep-review/].
`TODO: update new author count before submitting`
The Manubot system can also facilitate open research [@doi:10.7287/peerj.preprints.3100v1] in addition to review articles.
`TODO: get permission and add https://slochower.github.io/nonequilibrium-barrier/ https://zietzm.github.io/Vagelos2017/`

Our process represents an early step toward open massively collaborative reviews, and there are certainly aspects that can be improved.
We invite the scientific community to adapt and build upon our experience and open software.
